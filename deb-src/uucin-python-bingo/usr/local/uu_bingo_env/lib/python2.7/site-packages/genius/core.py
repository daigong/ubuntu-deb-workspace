#encoding:utf-8 

from .tools import StringHelper
from .loader import load_crf_seg_model,load_crf_pos_model
from .process import tokenizer_processes, tagger_processes

__all__ = ['CJKTokenizer', 'CJKTagger','Parser','Tokenizer']


class Parser(object):

    def __init__(self, model):
        self.model = model

    def parse(self,text):
        pass

class Tokenizer(Parser):

    def parse(self, text, use_break = True, use_combine = True):
        """
        text必须是unicode
        """
        if not text.strip():yield ""
        label = self.model.label_sequence(self.mark(text))#分词
        for best  in filter(lambda x : x,label.split("\n\n")):
            #返回初始分词结果
            result = tokenizer_processes['default'].process(best)
            if use_break:#对分词结构进行打断
                result = tokenizer_processes['use_break'].process(result)
            if use_combine:#合并分词结果
                result = tokenizer_processes['use_combine'].process(result)
            yield result

class CJKTokenizer(Tokenizer):

    def mark(self, word):
        groups = StringHelper.group_ascii_cjk(word)
        words = []
        for word in groups:
            mark = StringHelper.mark(word)
            if mark == 'ASCII':
                words.append(word)
            elif StringHelper.is_whitespace_string(word):
                words.append(r'\s')
            else:
                words.append('\n'.join([uchar for uchar in word]))
        return u'\n'.join(words)



class PinyinTokenizer(Tokenizer):

    def mark(self, text):
        return '\n'.join([uchar for uchar in text])


class CJKTagger(Parser):

    def mark(self,words):
        return ''.join([('%s\t%s\n' % (word, StringHelper.mark(word))) \
                for word in words])
    
    def parse(self, text):
        label = self.model.label_sequence(self.mark(text))
        for best in filter(lambda x:x,label.split('\n\n')):
            yield tagger_processes['default'].process(best)
