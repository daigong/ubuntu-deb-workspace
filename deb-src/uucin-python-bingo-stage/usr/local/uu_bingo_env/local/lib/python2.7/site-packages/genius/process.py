#encoding:utf-8 
import os
from .tools import StringHelper
from .loader import data_path

__all__ = ['tokenizer_processes', 'tagger_processes']


class SegAssemblingProcess(object):

    def process(self, word):
        result_words = []
        prev_words = []
        for word_label in filter(lambda x:x, word.split('\n')):
            word, label = word_label.split('\t')
            if 'S' in label or word == "\s":
                if prev_words:
                    result_words.append(u''.join(prev_words))
                    prev_words = []
                if word != "\s":
                    result_words.append(word)
            elif 'E' in label:
                prev_words.append(word)
                result_words.append(u''.join(prev_words))
                prev_words = []
            else:
                prev_words.append(word)
        return result_words


class SegBreakProcess(object):

    def __init__(self):
        break_idx = os.path.join(data_path, "break.txt")
        self.tree = {}
        if not os.path.exists(break_idx):
            return
        with open(break_idx) as break_file:
            for line in break_file:
                label = unicode(line,"utf-8").strip().split('\t')
                self.tree[label[0]] = label[1:]


    def process(self, word):
        break_word_result = []
        for term in word:
            if self.tree.has_key(term):
                break_word_result.extend(self.tree[term])
            else:
                break_word_result.append(term)
        return break_word_result


class SegCombineProcess(object):

    def __init__(self):
        combine_idx = os.path.join(data_path, "combine.txt")
        self.tree = set()
        if not os.path.exists(combine_idx):
            return
        with open(combine_idx) as combine_file:
            for line in combine_file:
                self.tree.add(unicode(line,"utf-8").strip())

    def process(self, word):
        pos = 0
        max_word_length = len(word)
        result_words = []
        while pos < (len(word)):
            pre_combine_word = ''
            for i in range(pos, max_word_length):
                combine_word = ''.join(word[pos:i+1])
                if combine_word in self.tree:
                    pre_combine_word += combine_word
                    pos = i + 1
            if pre_combine_word:
                result_words.append(pre_combine_word)
                pre_combine_word = None
            else:
                result_words.append(word[pos])
                pos += 1
        return result_words

tokenizer_processes = {
            'default':SegAssemblingProcess(),
            'use_break':SegBreakProcess(),
            'use_combine':SegCombineProcess(),
        }


class PosTaggerProcess(object):
    
    def process(self, words):
        result_words = []
        for word_label in filter(lambda x:x, words.split('\n')):
            result_words.append(word_label.split('\t'))
        return result_words

tagger_processes = {
            'default':PosTaggerProcess(),
        }
